# Wanda Gen-2 Communication, Voice, LLM & UI Architecture

Based on an analysis of Next-Gen agent frameworks, we are expanding Wanda with an architecture that prioritizes resilience, low latency, and maximum user transparency.

## 1. Multi-Channel Router (Features 1-6)
*   **The Problem:** Traditional bots tightly couple logic to the platform (e.g., replying natively within Telegram handler blocks). This makes cross-platform functionality a nightmare.
*   **Gen-2 Solution (The Universal Bus):** 
    *   Wanda Hub exposes an internal Event Emitter (`AgentBus`). 
    *   **Adapters** (Telegram via `grammY`, Discord via `discord.js`, WebChat via Websockets) act only as dumb pipes. They parse native payloads into a strict `UniversalMessage` format and fire it onto the bus.
    *   When Wanda replies, she targets a `recipient_uuid`. The Multi-Channel Router looks up which Adapter spawned that UUID and formats the response natively.

## 2. Always-On Voice Loop (Features 7-12)
*   **The Problem:** Voice in Text-LLMs is usually implemented via slow REST API calls, resulting in 5+ second latency, breaking the illusion of conversation.
*   **Gen-2 Solution (WebRTC + Streams):**
    *   **Audio ingest:** WebRTC captures audio in 50ms chunks, enabling Voice Activity Detection (VAD) "barge-in" (user interrupting the AI).
    *   **Transcribe:** Whisper-large-v3 (via Groq for speed or local).
    *   **Brain:** LLM generates tokens text.
    *   **Speak:** ElevenLabs *WebSocket streaming API*. We stream text tokens directly into ElevenLabs as they are generated by the LLM, and ElevenLabs streams audio back while the LLM is still typing.
    *   **Integration:** `Vox-Voice` will run as a child process or closely coupled service to ensure the heavy audio I/O doesn't block the main agent event loop.

## 3. LLM Orchestration & Failover (Features 21-27)
*   **The Problem:** Being locked to one provider (e.g., Anthropic) means if they have an outage, Wanda is dead.
*   **Gen-2 Solution (OpenRouter + Dynamic Fallback):**
    *   Every LLM call routes through an internal `LLM_Gateway`.
    *   The primary method uses **OpenRouter**, passing a `models` array: `["anthropic/claude-3.5-sonnet", "google/gemini-2.5-pro"]`. OpenRouter natively handles the automatic fast-fallback if Claude fails.
    *   If the user goes offline or privacy is required, the `LLM_Gateway` instantly hot-swaps the endpoint to a local **Ollama** instance defined in `settings.json`.

## 4. User-Centric UI Philosophy (Features 65 & "Big Focus for UI")
*   **The Problem:** AI agents operate as black boxes. Users don't know if the agent is stuck in a loop, what tools it is using, or what context it loaded.
*   **Gen-2 Solution (Layered Visibility Dashboard - Wanda MCC):**
    *   **Telemetry Stream:** The Wanda Hub continually blasts WebSocket telemetry to the `Wanda-MCC`.
    *   **Level 1 (Simple Mode):** The user sees a smooth typing indicator, an "Agent Thinking..." badge, and the final human-readable text output.
    *   **Level 2 (Deep Dive):** The user can expand the "Thinking" badge to see a live flow chart of memory queries, inner monologues, raw tool payloads (like executing a bash script), and token costs in real-time.
    *   **Settings Editor:** The MCC UI writes directly via API to the `settings.json` file inside the `Work-OS`, enabling the user to change Models, Keys, and Prompts visually.
